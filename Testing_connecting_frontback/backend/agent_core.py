# -*- coding: utf-8 -*-
"""Final_Version.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Fer_YRz5CysSUJP9hUMVDANpP_84JkHt
"""


try:
    import yfinance as yf
except Exception:
    yf = None

import pandas as pd
import numpy as np
import re

try:
    # transformers pieces used optionally at runtime
    from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
except Exception:
    pipeline = None
    AutoTokenizer = None
    AutoModelForCausalLM = None

from datetime import datetime, timedelta, date, timezone
import os
try:
    from dotenv import load_dotenv
except Exception:
    def load_dotenv(*a, **k):
        return None

try:
    import finnhub
except Exception:
    finnhub = None

import json
import textwrap
from zoneinfo import ZoneInfo
try:
    import torch
except Exception:
    torch = None

"""## Finhub"""

# -----------------------------
# Finnhub setup (API + client)
# -----------------------------

# Load .env if present (recommended)
load_dotenv()
FINNHUB_API_KEY = os.getenv("FINNHUB_API_KEY", "").strip()

finnhub_client = None
if finnhub is not None and FINNHUB_API_KEY:
    try:
        finnhub_client = finnhub.Client(api_key=FINNHUB_API_KEY)
    except Exception:
        finnhub_client = None

# -----------------------------
# Finnhub tools: news + fundamentals (+ optional quote)
# -----------------------------

def finnhub_quote(ticker: str) -> dict:
    """Latest quote from Finnhub (c=current, d=change, dp=% change, h/l/o/pc)."""
    if finnhub_client is None:
        return {"error": "finnhub_not_configured"}
    try:
        return finnhub_client.quote(ticker.upper())
    except Exception as e:
        return {"error": str(e)}
import re


# =====================================================
# Ticker validation (prevents words like STOCK, TONE)
# =====================================================
def is_valid_ticker_finnhub(t: str) -> bool:
    """
    Check if a ticker is a real tradable symbol using Finnhub.
    """
    try:
        q = finnhub_quote(t)
        # Finnhub returns empty or error dicts for invalid symbols
        return (
            isinstance(q, dict)
            and "error" not in q
            and q.get("c") is not None
        )
    except Exception:
        return False

#filter
def is_ticker_relevant(item: dict, ticker: str) -> bool:
    """
    Generic relevance filter:
    - keep if Finnhub 'related' contains the ticker token
    - OR headline/summary contains the ticker token
    """
    T = ticker.upper()
    related = (item.get("related") or "").upper()
    rel_tokens = {x.strip() for x in related.split(",") if x.strip()}
    text = " ".join([
        str(item.get("headline","")),
        str(item.get("summary","")),
        str(item.get("source",""))
    ]).upper()

    # match ticker as a token (avoid substrings)
    return (T in rel_tokens) or (re.search(rf"\b{re.escape(T)}\b", text) is not None)
def is_company_specific(item: dict, ticker: str) -> bool:
    """
    Keep only articles that look like they are truly about the company:
    headline must mention ticker OR company name.
    """
    if not item or "error" in item:
        return False

    T = ticker.upper()
    headline = (item.get("headline") or "").upper()

    # ticker token in headline
    if re.search(rf"\b{re.escape(T)}\b", headline):
        return True

    # map some common tickers to company names for headline matching
    COMPANY_NAMES = {
        "AAPL": "APPLE",
        "NVDA": "NVIDIA",
        "MSFT": "MICROSOFT",
        "AMZN": "AMAZON",
        "META": "META",
        "GOOGL": "ALPHABET",
        "TSLA": "TESLA",
        "AMD": "ADVANCED MICRO DEVICES",
    }
    cname = COMPANY_NAMES.get(T)
    if cname and cname in headline:
        return True

    return False

def finnhub_company_news(ticker: str, days=7, max_items=8, strict=True):
    """
    strict=True: keep only items where ticker appears in 'related'
    strict=False: return as-is (useful for debugging)
    """
    if finnhub_client is None:
        return []

    try:
        end = date.today()
        start = end - timedelta(days=days)

        items = finnhub_client.company_news(
            ticker,
            _from=start.isoformat(),
            to=end.isoformat()
        ) or []

        if not strict:
            return items[:max_items]

        filtered = [it for it in items if is_ticker_relevant(it, ticker)]

# If filtering removes everything, DO NOT fall back to all items
        if not filtered:
            return []   # <-- important
        return filtered[:max_items]


    except Exception as e:
        return [{"error": str(e)}]
def filter_items_to_date(items: list, target_d: date):
    """
    Keep only news items whose UTC date == target_d.
    """
    if not items:
        return items

    filtered = []
    for it in items:
        # pass through errors unchanged
        if isinstance(it, dict) and "error" in it:
            return items

        ts = it.get("datetime")
        if ts is None:
            continue

        item_date = datetime.utcfromtimestamp(ts).date()
        if item_date == target_d:
            filtered.append(it)

    return filtered
def finnhub_fundamentals_basic(ticker: str):
    """Basic financial metrics from Finnhub (best-effort)."""
    t = ticker.upper()
    try:
        resp = finnhub_client.company_basic_financials(t, "all") or {}
        metric = resp.get("metric", {}) if isinstance(resp, dict) else {}
        # Keep a compact, exam/demo-friendly subset
        keep = [
            "marketCapitalization",
            "peTTM",
            "pb",
            "epsTTM",
            "dividendYieldIndicatedAnnual",
            "52WeekHigh",
            "52WeekLow",
            "52WeekPriceReturnDaily",
            "beta",
        ]
        out = {k: metric.get(k, None) for k in keep}
        out["ticker"] = t
        return out
    except Exception as e:
        return {"ticker": t, "error": str(e)}
#new
def score_price_relevance(item: dict, ticker: str) -> int:
    """
    Heuristic score: higher means more likely to affect near-term price.
    """
    text = " ".join([
        str(item.get("headline","")),
        str(item.get("summary",""))
    ]).lower()

    score = 0

    # high-impact catalysts
    high = [
        "earnings", "guidance", "revenue", "profit", "miss", "beat",
        "sec", "doj", "ftc", "eu", "antitrust", "lawsuit", "appeals court", "ruling",
        "ban", "tariff", "sanction", "regulator", "investigation",
        "upgrade", "downgrade", "price target", "initiated", "rating",
        "buyback", "repurchase", "dividend", "split",
        "iphone", "sales", "shipment", "demand", "supply", "production"
    ]
    medium = [
        "forecast", "outlook", "estimate", "margin", "valuation",
        "analyst", "citi", "goldman", "morgan", "jpmorgan", "barclays",
        "etf", "buffett", "berkshire", "stake"
    ]

    for w in high:
        if w in text:
            score += 3
    for w in medium:
        if w in text:
            score += 1

    # bonus if headline explicitly mentions the company/ticker
    if ticker.lower() in text:
        score += 2

    return score

def score_company_specificity(item: dict, ticker: str) -> int:
    T = ticker.upper()
    headline = fix_mojibake(item.get("headline","") or "")
    summary  = fix_mojibake(item.get("summary","") or "")
    text = (headline + " " + summary).upper()

    score = 0

    # Explicit ticker token
    if re.search(rf"\b{re.escape(T)}\b", text):
        score += 6

    # Headline mentions ticker/company strongly (ticker token)
    if re.search(rf"\b{re.escape(T)}\b", headline.upper()):
        score += 4

    # Penalize generic "portfolio/yield/eod" style pieces
    junk = ["EOD:", "PORTFOLIO", "YIELD", "INCOME", "BUY AND HOLD"]
    if any(j in headline.upper() for j in junk):
        score -= 4

    return score

def filter_price_relevant_news(items: list, ticker: str, max_items: int = 8, min_score: int = 3):
    scored = []
    for it in items:
        if "error" in it:
            continue
        s = score_price_relevance(it, ticker)
        scored.append((s, it))

    scored.sort(key=lambda x: x[0], reverse=True)

    # keep those above threshold; if none, fall back to top-scored anyway
    strong = [it for s, it in scored if s >= min_score]
    if strong:
        return strong[:max_items]
    return [it for s, it in scored][:max_items]

def format_news_items(items):
    lines = []
    for it in items:
        if "error" in it:
            lines.append(f"âš ï¸ Finnhub news error: {it['error']}")
            break
        ts = it.get("datetime", None)
        date_str = datetime.fromtimestamp(ts, timezone.utc).strftime("%Y-%m-%d %H:%M UTC") if ts else "N/A"
        headline = it.get("headline", "").strip()
        source = it.get("source", "").strip()
        url = it.get("url","").strip()
        lines.append(f"- [{date_str}] {headline} ({source})\n  {url}")
    return "\n".join(lines) if lines else "No recent news returned."

def format_sentiment_human(sent: dict, ticker: str = "", when: str = "recent") -> str:
    if not sent or "sentiment" not in sent:
        return "Sentiment data unavailable."
    if "sentiment" not in sent and "label" in sent:
        sent = dict(sent)
        sent["sentiment"] = sent["label"]

    if "sentiment" not in sent:
        return "Sentiment data unavailable."

    pos = int(sent.get("positive_count", 0) or 0)
    neg = int(sent.get("negative_count", 0) or 0)
    neu = int(sent.get("neutral_count", 0) or 0)
    total = int(sent.get("article_count", 0) or 0)

    if total == 0:
        label = f"{ticker} " if ticker else ""
        return f"ðŸ§  News sentiment for {label}({when})\n\nNo recent articles available for sentiment analysis."

    score = float(sent.get("score", 0.0) or 0.0)

    if score >= 0.20:
        tone = "Mostly positive"
        explanation = "Most coverage has been favorable."
    elif score <= -0.20:
        tone = "Mostly negative"
        explanation = "Most coverage has focused on risks or concerns."
    else:
        tone = "Mixed / neutral"
        explanation = "Coverage is balanced with both positive and negative viewpoints."


    # â€œconfidenceâ€ in your analyzer is std dev of article scores (dispersion)
    dispersion = float(sent.get("dispersion", 0.0) or 0.0)
    if dispersion < 0.10:
        conf = "High"
    elif dispersion < 0.25:
        conf = "Medium"
    else:
        conf = "Low"

    label = f"{ticker} " if ticker else ""
    return (
        f"ðŸ§  News sentiment for {label}({when})\n\n"
        f"Overall tone: {tone} (score {score:+.3f})\n\n"
        f"What this means:\n"
        f"â€¢ {explanation}\n"
        f"â€¢ Breakdown: {pos} positive / {neg} negative / {neu} neutral (out of {total} articles)\n\n"
        f"Confidence: {conf}"
    )
def format_fundamentals(d: dict):
    if not d:
        return "No fundamentals returned."
    if "error" in d:
        return f"âš ï¸ Finnhub fundamentals error: {d['error']}"
    def fmt(v):
        if v is None:
            return "N/A"
        try:
            if isinstance(v, (int,float)):
                return f"{v:,.4g}"
        except Exception:
            pass
        return str(v)
    keys = [k for k in d.keys() if k != "ticker"]
    lines = [f" Finnhub fundamentals for {d.get('ticker','')} (selected metrics):"]
    for k in keys:
        lines.append(f"- {k}: {fmt(d.get(k))}")
    return "\n".join(lines)

#new
def finnhub_company_news_range(ticker: str, start_d: date, end_d: date, max_items: int = 30):
    """
    Fetch company news in [start_d, end_d] inclusive-ish.
    Finnhub API uses 'to' as a date string; we pass end_d.
    """
    if finnhub_client is None:
        return []
    try:
        items = finnhub_client.company_news(
            ticker.upper(),
            _from=start_d.isoformat(),
            to=end_d.isoformat()
        ) or []
        return items[:max_items]
    except Exception as e:
        return [{"error": str(e)}]

"""# LLM"""


# Load locally-fine-tuned model if available. Fail gracefully at import-time.
tokenizer = None
model = None
if AutoTokenizer is not None and AutoModelForCausalLM is not None:
    try:
        tokenizer = AutoTokenizer.from_pretrained("./model_final")
        model = AutoModelForCausalLM.from_pretrained(
            "./model_final",
            device_map="auto",
            dtype="auto"
        )
        try:
            print("âœ… fine-tuned model loaded")
        except Exception:
            pass
    except Exception:
        tokenizer = None
        model = None
        print("âš ï¸ Local model './model_final' not available at import time.")
else:
    print("âš ï¸ transformers not available; LLM features disabled at import time.")

"""##test for fine-tuning"""

# Run quick local LLM sanity checks only when executed as a script
# and when tokenizer/model are actually available.
if __name__ == "__main__" and tokenizer is not None and model is not None:
    prompt = "Analyze sentiment: Apple faces antitrust scrutiny."

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=20, do_sample=False)

    print(tokenizer.decode(outputs[0], skip_special_tokens=True))

    prompt = """Analyze sentiment: Apple faces antitrust scrutiny.
    Answer with one word only.
    Sentiment:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(
        **inputs,
        max_new_tokens=5,
        do_sample=False,
        eos_token_id=tokenizer.encode("\n")[0]
    )

    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
else:
    # Skip demo runs at import time
    pass

"""## LLM sentiment reasoning"""

#new
def llm_sentiment_reasoning(
    items: list,
    ticker: str,
    llm_model,
    tokenizer
) -> str:
    if not items:
        return "No recent news available for sentiment analysis."


    snippets = []
    for it in items[:6]:
        h = it.get("headline", "")
        s = it.get("summary", "")
        snippets.append(f"- {h}. {s}")

    news_block = "\n".join(snippets)

    prompt = f"""
Analyze the overall sentiment for {ticker} based on the following news.

Rules:
- Output ONE of: Positive / Negative / Neutral
- Then give a brief explanation (2â€“3 sentences max)
- Focus on market impact, not storytelling

News:
{news_block}

Answer format:
Sentiment: <label>
Reasoning: <short explanation>
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(llm_model.device)
    outputs = llm_model.generate(
        **inputs,
        max_new_tokens=120,
        do_sample=False
    )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if "Sentiment:" in text:
        return text.split("Sentiment:", 1)[-1].strip()
    return text.strip()

"""## Price Interpretationï¼ˆLLMï¼‰"""

ANSWER_TAG = "<<<ANSWER>>>"

def llm_price_interpretation(snapshot, model, tokenizer, max_tokens=120):
    prompt = f"""
You are a market analyst.

Given the following price snapshot for {snapshot['ticker']},
explain briefly what it implies.

Rules:
- Do NOT predict future prices
- Do NOT invent catalysts
- Focus on market context (noise vs signal)
- Do NOT mention indicators unless explicitly present

Price snapshot:
{snapshot['raw_output']}

Respond with 2â€“3 sentences only.
Put each sentence on its own line.
Do NOT number the sentences.

{ANSWER_TAG}
"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    outputs = model.generate(
        **inputs,
        max_new_tokens=max_tokens,
        do_sample=False,
        pad_token_id=tokenizer.eos_token_id
    )

    full_text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    if ANSWER_TAG in full_text:
        answer = full_text.split(ANSWER_TAG, 1)[1].strip()
    else:

        answer = full_text.strip()


    lines = [l.strip() for l in answer.splitlines() if l.strip()]
    return "\n".join(lines[:3])

"""## LLM_complete"""

def llm_complete(
    prompt: str,
    max_new_tokens: int = 60
) -> str:
    if model is None or tokenizer is None or torch is None:
        raise RuntimeError("LLM not available (missing ./model_final or torch/transformers).")

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,
            temperature=0.0,
            repetition_penalty=1.0,
            eos_token_id=tokenizer.encode("\n")[0],
            pad_token_id=tokenizer.eos_token_id
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

"""# FinBERT"""

finbert = None
if pipeline is not None:
    try:
        finbert = pipeline(
            "text-classification",
            model="ProsusAI/finbert",
            top_k=None
        )
    except Exception:
        finbert = None

"""# Stock price"""

def fetch_stock_data(ticker, period="6mo"):
    df = yf.download(
        ticker,
        period=period,
        interval="1d",
        auto_adjust=False,
        progress=False
    )


    if isinstance(df.columns, pd.MultiIndex):
        df.columns = df.columns.get_level_values(0)


    df.index = pd.to_datetime(df.index)

    df["Return"] = df["Close"].pct_change()
    df["MA20"] = df["Close"].rolling(20).mean()
    df["Volatility"] = df["Return"].rolling(20).std()

    return df.dropna()

def summarize_metrics(df):
    close_last = df["Close"].iloc[-1]
    close_first = df["Close"].iloc[0]
    ma_last = df["MA20"].iloc[-1]

    return {
        "price": float(close_last),
        "return_6m": float(close_last / close_first - 1),
        "volatility": float(df["Volatility"].mean()),
        "above_ma": bool(close_last > ma_last)
    }

def get_last_price_fastinfo(ticker: str):
    try:
        fi = yf.Ticker(ticker).fast_info
        return fi.get("last_price", None)
    except Exception:
        return None

import yfinance as yf
from datetime import datetime, time

def price_now(ticker: str, mode: str = "now"):
    """
    Unified price interface.

    mode:
    - "close" â†’ official daily close (4:00 PM ET)
    - "now"   â†’ last traded price if available, otherwise close
    - "debug" â†’ show ALL available price sources (for inspection)
    """

    t = yf.Ticker(ticker)
    lines = [f"ðŸ’° {ticker} price snapshot:"]

    # =====================================================
    # 1) OFFICIAL DAILY CLOSE (authoritative)
    # =====================================================
    close_price = None
    close_date = None

    try:
        daily = t.history(period="2d", interval="1d")
        if daily is not None and not daily.empty:
            close_price = float(daily["Close"].iloc[-1])
            close_date = daily.index[-1].date()
            lines.append(
                f"- Official close ({close_date} 16:00 ET): ${close_price:.2f}"
            )
    except Exception:
        pass

    # =====================================================
    # 2) LAST TRADED PRICE (near real-time)
    # =====================================================
    last_price = None
    try:
        fi = t.fast_info or {}
        last_price = fi.get("last_price", None)
    except Exception:
        last_price = None

    if mode in {"now", "debug"}:
        if last_price is not None:
            # avoid redundant noise unless debug
            if (
                mode == "debug"
                or close_price is None
                or abs(last_price - close_price) > 1e-4
            ):
                lines.append(
                    f"- Last traded price (near real-time): ${last_price:.2f}"
                )

    # =====================================================
    # 3) AFTER-HOURS PRICE (best-effort)
    # =====================================================
    after_hours_price = None
    after_hours_ts = None

    try:
        intraday = t.history(period="1d", interval="1m")
        if intraday is not None and not intraday.empty:
            last_bar = intraday.iloc[-1]
            ts = intraday.index[-1]

            # if trade happened after 16:00 ET
            if ts.time() > time(16, 0):
                after_hours_price = float(last_bar["Close"])
                after_hours_ts = ts
    except Exception:
        pass

    if mode in {"now", "debug"}:
        if after_hours_price is not None:
            lines.append(
                f"- After-hours trade ({after_hours_ts.strftime('%Y-%m-%d %H:%M ET')}): "
                f"${after_hours_price:.2f}"
            )

    # =====================================================
    # 4) DEBUG: show raw fast_info
    # =====================================================
    if mode == "debug":
        lines.append("")
        lines.append("ðŸ” Debug info:")
        try:
            for k, v in (t.fast_info or {}).items():
                lines.append(f"  {k}: {v}")
        except Exception:
            lines.append("  fast_info unavailable")

    # =====================================================
    # 5) SAFETY FALLBACK
    # =====================================================
    if len(lines) == 1:
        return f"âŒ Could not fetch price data for {ticker}."

    return "\n".join(lines)

def get_price_snapshot_structured(ticker: str, mode: str = "price_now") -> dict:
    """
    Machine-readable price snapshot for LLM interpretation.
    """
    t = yf.Ticker(ticker)

    daily = t.history(period="5d", interval="1d")
    daily = _normalize_yf_download(daily)

    if daily is None or daily.empty:
        return {"error": "price_unavailable", "ticker": ticker}

    close_price = float(daily["Close"].iloc[-1])
    close_date  = daily.index[-1].date()

    ma20 = daily["Close"].rolling(20).mean().iloc[-1] if len(daily) >= 20 else None
    volatility = daily["Close"].pct_change().std()

    return {
        "ticker": ticker,
        "close_price": close_price,
        "close_date": str(close_date),
        "above_ma20": (close_price > ma20) if ma20 else None,
        "volatility": float(volatility) if volatility else None,
    }

def parse_date_from_text(text: str):
    """
    Parse dates like:
      - Dec 11th 2025
      - Dec 11 2025
      - 2025-12-11
      - 12/11/2025
    Return pd.Timestamp or None
    """
    t = text.strip()

    # Normalize "11th" -> "11"
    t = re.sub(r'(\d+)(st|nd|rd|th)', r'\1', t, flags=re.IGNORECASE)

    # Try common formats via pandas
    # (pandas can parse "Dec 11 2025" well)
    try:
        dt = pd.to_datetime(t, errors="coerce")
        if pd.isna(dt):
            return None
        return dt.normalize()
    except Exception:
        return None


def extract_date(user_query: str):
    """
    Extract a date substring from the query, then parse it.
    """
    # Try YYYY-MM-DD
    m = re.search(r"\b\d{4}-\d{2}-\d{2}\b", user_query)
    if m:
        return parse_date_from_text(m.group(0))

    # Try MM/DD/YYYY
    m = re.search(r"\b\d{1,2}/\d{1,2}/\d{4}\b", user_query)
    if m:
        return parse_date_from_text(m.group(0))

    # Try "Dec 11 2025" / "December 11 2025" (with optional suffix)
    m = re.search(r"\b(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Sept|Oct|Nov|Dec)[a-z]*\s+\d{1,2}(?:st|nd|rd|th)?\s+\d{4}\b",
                  user_query, flags=re.IGNORECASE)
    if m:
        return parse_date_from_text(m.group(0))

    return None

import pandas as pd
import yfinance as yf

def _normalize_yf_download(df: pd.DataFrame) -> pd.DataFrame:

    if df is None or df.empty:
        return df

    if isinstance(df.columns, pd.MultiIndex):

        df.columns = df.columns.get_level_values(0)

    return df

def get_daily_ohlcv_fields_on_date(ticker, date, fields, auto_adjust=False):

    date = pd.to_datetime(date)
    start = date - pd.Timedelta(days=7)
    end   = date + pd.Timedelta(days=7)

    df = yf.download(
        ticker,
        start=start.strftime("%Y-%m-%d"),
        end=end.strftime("%Y-%m-%d"),
        progress=False,
        auto_adjust=auto_adjust
    )

    df = _normalize_yf_download(df)
    if df is None or df.empty:
        return None, None

    df2 = df.copy()
    df2["__date"] = df2.index.date
    target = date.date()

    row = df2[df2["__date"] == target]
    if row.empty:

        row = df2[df2["__date"] <= target].tail(1)
        if row.empty:
            return None, None

    used_date = pd.to_datetime(row.index[0]).to_pydatetime()

    out = {}
    for f in fields:
        if f == "Price":

            out["Price"] = float(row["Close"].iloc[0])
        elif f in row.columns:
            out[f] = float(row[f].iloc[0])
        else:
            out[f] = None

    return used_date, out

def _safe_float(x):
    try:
        if x is None:
            return None
        if isinstance(x, (pd.Series, pd.DataFrame)):
            return None
        return float(x)
    except Exception:
        return None

def get_today_intraday_fields(ticker, fields):
    t = yf.Ticker(ticker)

    # intraday 1m (best for today's H/L + last)
    intraday = t.history(period="1d", interval="1m")
    if intraday is None or intraday.empty:
        intraday = None

    # daily fallback
    daily = t.history(period="5d", interval="1d")
    if daily is None or daily.empty:
        daily = None

    # last price (priority: fast_info -> intraday last close -> daily last close)
    last = None
    try:
        fi = getattr(t, "fast_info", {}) or {}
        last = _safe_float(fi.get("last_price", None))
    except Exception:
        last = None

    if last is None and intraday is not None and "Close" in intraday.columns:
        last = _safe_float(intraday["Close"].iloc[-1])

    if last is None and daily is not None and "Close" in daily.columns:
        last = _safe_float(daily["Close"].iloc[-1])

    # open/high/low today
    open_today = None
    high_today = None
    low_today = None
    vol_today = None

    if intraday is not None:
        if "Open" in intraday.columns:
            open_today = _safe_float(intraday["Open"].iloc[0])
        if "High" in intraday.columns:
            high_today = _safe_float(intraday["High"].max())
        if "Low" in intraday.columns:
            low_today = _safe_float(intraday["Low"].min())
        if "Volume" in intraday.columns:
            vol_today = _safe_float(intraday["Volume"].sum())

    # fallback to daily last row if intraday missing pieces
    if daily is not None:
        if open_today is None and "Open" in daily.columns:
            open_today = _safe_float(daily["Open"].iloc[-1])
        if high_today is None and "High" in daily.columns:
            high_today = _safe_float(daily["High"].iloc[-1])
        if low_today is None and "Low" in daily.columns:
            low_today = _safe_float(daily["Low"].iloc[-1])
        if vol_today is None and "Volume" in daily.columns:
            vol_today = _safe_float(daily["Volume"].iloc[-1])

    out = {}
    for f in fields:
        if f == "Open":
            out[f] = open_today
        elif f == "High":
            out[f] = high_today
        elif f == "Low":
            out[f] = low_today
        elif f in ["Close", "Price"]:
            out[f] = last
        elif f == "Volume":
            out[f] = vol_today
        else:
            out[f] = None

    return out

def pick_primary_ticker(tickers: list[str], query: str | None = None) -> str:
    """
    Pick the most plausible ticker:
    1) If any extracted ticker appears explicitly in the raw query, prefer it.
    2) Otherwise pick the first Finnhub-valid ticker.
    3) Otherwise fall back to first candidate.
    """
    if not tickers:
        return None

    # 1) Prefer ticker explicitly present in the query text
    if query:
        q = query.upper()
        for t in tickers:
            if re.search(rf"\b{re.escape(t.upper())}\b", q):
                return t

    # 2) Prefer real tickers
    for t in tickers:
        if is_valid_ticker_finnhub(t):
            return t

    # 3) Fallback
    return tickers[0]

def get_today_price_change(ticker: str):
    """
    Returns today's last price and % change vs previous close.
    """
    try:
        df = yf.download(
            ticker,
            period="2d",
            interval="1d",
            auto_adjust=False,
            progress=False
        )

        if df is None or df.empty or len(df) < 2:
            return None

        prev_close = df["Close"].iloc[-2].item()
        last_price = df["Close"].iloc[-1].item()

        pct_change = (last_price - prev_close) / prev_close * 100

        return {
            "price": round(float(last_price), 2),
            "pct_change": round(float(pct_change), 2)
        }

    except Exception:
        return None

"""# Stock Compare"""

def compare_basic(tickers, period="6mo"):
    rows = []
    for t in tickers:
        df = fetch_stock_data(t, period)
        if df is None or df.empty:
            continue
        close = df["Close"]
        rows.append({
            "ticker": t,
            "return": close.iloc[-1] / close.iloc[0] - 1,
            "volatility": df["Volatility"].mean(),
            "max_drawdown": compute_max_drawdown(close),
        })
    return pd.DataFrame(rows)

def generate_relative_verdict(ticker_a, a_tech, a_sent,
                              ticker_b, b_tech, b_sent):
    """
    Generate PM-style relative verdict between two stocks.
    Inputs are already-computed summaries (NO LLM).
    """

    def tech_score(tech):
        score = 0

        # Trend
        trend = tech.get("trend")
        if trend == "Bullish":
            score += 2
        elif trend == "Improving":
            score += 1
        elif trend == "Weak":
            score -= 1

        # RSI
        rsi = tech.get("rsi14", 50)
        if rsi >= 60:
            score += 1
        elif rsi <= 40:
            score -= 1

        # Drawdown
        mdd = tech.get("max_drawdown", 0)
        if mdd >= -0.10:
            score += 1
        elif mdd <= -0.25:
            score -= 1

        return score

    def sent_score(sent):
        return sent.get("score", 0.0)

    a_score = tech_score(a_tech) + sent_score(a_sent)
    b_score = tech_score(b_tech) + sent_score(b_sent)

    if a_score > b_score:
        winner, loser = ticker_a, ticker_b
        w_score, l_score = a_score, b_score
    elif b_score > a_score:
        winner, loser = ticker_b, ticker_a
        w_score, l_score = b_score, a_score
    else:
        return (
            "ðŸ“Œ Relative Verdict\n\n"
            "No clear relative preference at this time.\n"
            "Both stocks exhibit comparable technical and sentiment profiles."
        )

    return f"""ðŸ“Œ Relative Verdict (PM-style)

Preferred: {winner}
Relative Laggard: {loser}

Rationale:
- {winner} shows stronger technical structure and/or better risk-adjusted profile.
- Sentiment signals do not offset the technical advantage.

Risk Considerations:
- Trend deterioration or sentiment shock could invalidate the relative view.

Score Summary:
- {winner}: {w_score:.2f}
- {loser}: {l_score:.2f}

What would change the view:
- Breakdown of key technical levels for {winner}
- Confirmed trend reversal or positive catalyst for {loser}
""".strip()

def format_compare_table(rows):
    """
    rows: list of dicts with identical keys
    """
    if not rows:
        return ""

    headers = list(rows[0].keys())

    # column widths
    col_w = {
        h: max(len(h), max(len(str(r[h])) for r in rows))
        for h in headers
    }

    def fmt_row(r):
        return " | ".join(
            str(r[h]).ljust(col_w[h]) for h in headers
        )

    sep = "-+-".join("-" * col_w[h] for h in headers)

    lines = []
    lines.append(fmt_row({h: h for h in headers}))
    lines.append(sep)
    for r in rows:
        lines.append(fmt_row(r))

    return "\n".join(lines)

def score_technical_sentiment(tech: dict, sent: dict) -> float:
    """
    Simple PM-style scoring.
    Positive = better.
    """

    score = 0.0

    # ---- Trend ----
    trend = tech.get("trend", "")
    if trend == "Bullish":
        score += 1.0
    elif trend == "Improving":
        score += 0.5
    elif trend == "Weak":
        score -= 0.5

    # ---- RSI ----
    rsi = tech.get("rsi14", 50)
    if rsi >= 60:
        score += 0.5
    elif rsi <= 40:
        score -= 0.5

    # ---- Drawdown ----
    mdd = tech.get("max_drawdown", 0.0)
    if mdd <= -0.25:
        score -= 0.5
    elif mdd >= -0.10:
        score += 0.3

    # ---- Sentiment ----
    sent_score = sent.get("score", 0.0)
    score += sent_score  # already small magnitude

    return round(score, 2)

"""# extract ticker"""

INTENT_WORDS = {
    # core intents
    "PRICE", "PRICES",
    "RETURN", "RETURNS",
    "RISK", "RISKS",
    "VOLUME", "VOLUMES",
    "SENTIMENT", "TONE",
    "NEWS", "HEADLINE", "HEADLINES",
    "ANALYSIS", "ANALYZE",
    "TECHNICAL", "TECHNICALS",

    # question words
    "WHY", "WHAT", "HOW", "WHEN", "WHERE", "WHO",

    # technical indicators / structure
    "RSI", "MACD", "VWAP", "MA", "EMA", "SMA",
    "TREND", "SETUP", "STRUCTURE", "VIEW", "PERSPECTIVE",

    # time horizon
    "SHORT", "LONG", "MEDIUM", "TERM",

    # common false positives
    "ME", "LOOK", "LIKE"
}

_TICKER_STOP = {
     # English function words
    "ABOUT", "WITH", "WITHOUT", "FOR", "FROM", "TO", "ON", "IN", "OF",

    # Query words
    "NEWS", "LATEST", "TODAY", "PRICE", "PRICES", "WHY", "HOW",

    # Compare words
    "VS", "VERSUS", "COMPARE",

    # Time
    "NOW", "RECENT",

    # Sentiment / analysis
    "SENTIMENT", "TECHNICAL", "FUNDAMENTAL", "FUNDAMENTALS",

    # Generic
    "STOCK", "STOCKS", "COMPANY", "MARKET",

    # verbs / commands
    "CHECK","SHOW","ANALYZE","ANALYSIS","DISCUSS","EXPLAIN","GIVE","TELL",

    # grammar
    "IS","ARE","WAS","WERE","BE","BEEN","BEING",
    "HAS","HAVE","HAD","DO","DOES","DID",

    # relations
    "ABOVE","BELOW","OVER","UNDER","BETWEEN","AROUND","VS","VERSUS",

    # common language
    "THE","A","AN","AND","OR","OF","ON","IN","FOR","TO","FROM","WITH","ITS",

    # time
    "DAY","DAYS","MONTH","MONTHS","YEAR","YEARS","TODAY","NOW",

    # finance words (not tickers)
    "PRICE","RETURN","RETURNS","VOLATILITY",
    "OPEN","HIGH","LOW","CLOSE","VOLUME",

    # news words
    "NEWS","HEADLINE","HEADLINES",
    "STORY","STORIES","ARTICLE","ARTICLES",
    "UPDATE","UPDATES",

    # macro / generic
    "ETF","IPO","CEO","EPS","GDP","FED",
}

_ALIAS = {
    "APPLE": "AAPL",
    "TESLA": "TSLA",
    "NVIDIA": "NVDA",
    "META": "META",
    "FACEBOOK": "META",
    "MICROSOFT": "MSFT",
    "AMAZON": "AMZN",
    "GOOGLE": "GOOGL",
    "ALPHABET": "GOOGL",
}

_ETF_WHITELIST = {
    "SPY", "QQQ", "DIA", "IWM",
    "XLK", "XLF", "XLE", "XLV"
}

def llm_extract_tickers_fallback(query: str) -> list[str]:
    """
    epoch_5-powered ticker extractor.
    ONLY called when rule-based extraction fails.
    """

    prompt = f"""
You are a financial NER system.

Task:
Extract US stock tickers from the user query.

Rules:
- Return JSON only
- No explanation
- Do NOT guess
- Return [] if unsure

Query:
"{query}"

Output:
{{"tickers": []}}
"""

    try:
        raw = llm_complete(prompt, max_new_tokens=30)
        data = json.loads(raw)
        tickers = data.get("tickers", [])
    except Exception:
        return []

    clean = []
    for t in tickers:
        t = t.upper()
        if (
            re.fullmatch(r"[A-Z]{1,5}", t)
            and t not in _TICKER_STOP
            and is_valid_ticker_finnhub(t)
        ):
            clean.append(t)

    return clean

# =================================================
# MAIN EXTRACTOR
# =================================================

def extract_tickers(query: str) -> list[str]:
    """
    Production-grade ticker extractor.

    Principles:
    - Rule-first
    - No guessing
    - Hard real-world validation
    - LLM only as last resort
    """

    if not query or not isinstance(query, str):
        return []

    # ----------------------------
    # 0) System / prompt guard
    # ----------------------------
    q_raw = query.strip().lower()
    if q_raw.startswith(("you are", "system:", "assistant:", "rules:")):
        return []

    # ----------------------------
    # 1) Normalize & intent scrub
    # ----------------------------
    q = query.upper()

    for w in INTENT_WORDS:
        q = re.sub(rf"\b{w}\b", " ", q)

    q = re.sub(r"\s+", " ", q).strip()
    if not q:
        return []

    # ----------------------------
    # 2) Remove numbering
    # ----------------------------
    q = re.sub(r"^\s*\d+[\.\)]\s*", "", q)

    # ----------------------------
    # 3) Explicit $AAPL
    # ----------------------------
    dollar = re.findall(r"\$([A-Z]{1,5})\b", q)
    if dollar:
        return list(dict.fromkeys(dollar))

    # ----------------------------
    # 4) Company aliases (highest confidence)
    # ----------------------------
    for name, ticker in _ALIAS.items():
        if re.search(rf"\b{name}\b", q):
            return [ticker]

    # ----------------------------
    # 5) Uppercase token scan
    # ----------------------------
    tokens = re.findall(r"\b[A-Z]{2,5}\b", q)

    candidates = []
    for t in tokens:
        if t in INTENT_WORDS or t in _TICKER_STOP:
            continue
        if t.endswith(("ED", "ING")):
            continue
        if t in _ETF_WHITELIST:
            candidates.append(t)
            continue
        if not is_valid_ticker_finnhub(t):
            continue
        candidates.append(t)

    # ----------------------------
    # 6) Final hard validation
    # ----------------------------
    final = []
    for t in candidates:
        if is_valid_ticker_finnhub(t):
            final.append(t)

    final = list(dict.fromkeys(final))
    if final:
        return final

    # ----------------------------
    # 7) LAST resort: LLM fallback
    # ----------------------------
    # âš ï¸ LLM fallback is intentionally LAST resort
    return llm_extract_tickers_fallback(query)



# -----------------------------
# 2) field extraction
#   - "price" -> Price
#   - "close/closing" -> Close
# -----------------------------


FIELD_SYNONYMS = {
    "Open":   ["open", "opening"],
    "High":   ["high", "highest"],
    "Low":    ["low", "lowest"],
    "Close":  ["close", "closing", "settle", "settlement"],
    "Volume": ["volume", "vol", "shares"],
    "Price":  ["price", "last", "trading", "quote", "current"],
}

def extract_fields(query: str):
    """
    Return list like ["High","Low"] or None.
    Supports asking multiple fields in one question.
    """
    if not query:
        return None

    q = query.lower()
    found = []

    for field, keys in FIELD_SYNONYMS.items():
        if any(k in q for k in keys):
            found.append(field)

    # shorthand
    if "ohlcv" in q:
        for f in ["Open","High","Low","Close","Volume"]:
            if f not in found:
                found.append(f)
    elif "ohlc" in q:
        for f in ["Open","High","Low","Close"]:
            if f not in found:
                found.append(f)

    return found if found else None

"""# detect_intent"""

INTENT_LABELS = [
    "price_now",
    "news",
    "sentiment",
    "fundamentals",
    "technical_full",
    "technical_indicator",
    "technical_ma_check",
    "technical_trend",
    "technical_risk",
    "return",
    "compare",
    "bull",
    "bear",
    "risk",
    "analysis"
]

import torch

def predict_intent_with_epoch5(
    query: str,
    model,
    tokenizer,
    intent_labels=INTENT_LABELS,
    max_new_tokens: int = 4
) -> str:
    """
    Deterministic intent classifier using fine-tuned epoch_5.

    - NO free generation
    - HARD stop
    - One label only
    """
    if model is None or tokenizer is None or torch is None:
        return "analysis"

    prompt = (
        "You are a financial intent classifier.\n\n"
        "Choose exactly ONE intent from this list:\n"
        + ", ".join(intent_labels)
        + "\n\n"
        f"User query:\n{query}\n\n"
        "Intent:"
    )

    inputs = tokenizer(
        prompt,
        return_tensors="pt",
        truncation=True,
        max_length=256
    ).to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=False,                 # âŒ no sampling
            temperature=0.0,                 # âŒ no randomness
            repetition_penalty=1.0,
            eos_token_id=tokenizer.encode("\n")[0],  # âœ… HARD STOP
            pad_token_id=tokenizer.eos_token_id
        )

    text = tokenizer.decode(outputs[0], skip_special_tokens=True)

    # ----------------------------
    # Parse the LAST token chunk
    # ----------------------------
    tail = text.split("Intent:", 1)[-1].strip()
    pred = tail.split()[0].lower()

    if pred in intent_labels:
        return pred

    # -------- HARD fallback (never crash agent) --------
    return "analysis"

NEWS_WORDS = {
    "news", "headline", "headlines",
    "article", "articles",
    "earnings", "guidance",
    "report", "reports"
}

SENT_WORDS = {
    "sentiment", "tone", "positive or negative",
    "bullish", "bearish"
}

def detect_multi_intent(query: str):
    q = (query or "").lower()

    intents = set()
    flags = {
        "explain": False,
        "why": False,
        "compare": False,
        "technical": False,
    }
    # =====================================================
    # ðŸ§  VIEWPOINT AGENTS (HIGHEST PRIORITY)
    # =====================================================

    if any(k in q for k in [
        "bullish thesis", "bull case", "bull view", "bullish on"
    ]):
        return {
            "primary": "bull",
            "secondary": [],
            "flags": {}
        }

    if any(k in q for k in [
        "bearish thesis", "bear case", "bear view", "downside risks"
    ]):
        return {
            "primary": "bear",
            "secondary": [],
            "flags": {}
        }

    if any(k in q for k in [
        "risk", "key risks", "risk profile", "holding risk"
    ]):
        return {
            "primary": "risk",
            "secondary": [],
            "flags": {}
        }


    # -------------------------
    # EXPLAIN / WHY flags
    # -------------------------
    if any(k in q for k in ["explain", "interpret", "what does it mean"]):
        flags["explain"] = True

    if any(k in q for k in ["why", "reason", "because", "behind", "what is driving"]):
        flags["why"] = True
        flags["explain"] = True

    # -------------------------
    # COMPARE
    # -------------------------
    is_compare = any(k in q for k in [" vs ", "versus", "compare", "comparison"])
    if is_compare:
        flags["compare"] = True
        intents.add("compare")

    # =====================================================
    # TECHNICAL (DETECT FIRST, STRONG SIGNAL)
    # =====================================================
    TECH_WORDS = [
        "technical", "structure", "setup",
        "trend", "rsi", "moving average",
        "support", "resistance", "momentum",
        "risk", "drawdown", "volatility",
        # âœ… intraday
        "intraday", "today", "vwap", "1m", "5m", "15m"
    ]

    is_technical = any(k in q for k in TECH_WORDS)

    if is_technical:
        flags["technical"] = True

        # âœ… intraday-level
        if any(k in q for k in ["intraday", "today", "vwap", "1m", "5m", "15m"]):
            intents.add("technical_intraday")

        # -------- indicator-level --------
        elif any(k in q for k in ["rsi", "macd", "momentum", "overbought", "oversold"]):
            intents.add("technical_indicator")

        # -------- risk profile --------
        elif any(k in q for k in ["risk", "drawdown", "volatility"]):
            intents.add("technical_risk")

        # -------- trend / structure --------
        elif any(k in q for k in ["trend", "setup", "structure", "medium-term", "bias"]):
            intents.add("technical_trend")

        # -------- fallback: full technical --------
        else:
            intents.add("technical_full")

    # -------------------------
    # PRICE
    # -------------------------
    has_news = any(w in q for w in NEWS_WORDS)

    if (
        not has_news
        and not is_technical
        and any(k in q for k in [
            "price", "how much", "current", "now", "latest", "last"
        ])
    ):
        intents.add("price_now")

    # -------------------------
    # SENTIMENT / NEWS
    # -------------------------
    wants_sent = any(k in q for k in SENT_WORDS)
    wants_news = any(k in q for k in [
        "news", "headline", "earnings", "guidance",
        "reaction", "impact"
    ])

    if wants_sent and (is_compare or (not is_technical)):
        intents.add("sentiment")

    if wants_news and (is_compare or (not is_technical)):
        intents.add("news")

    # -------------------------
    # FALLBACK
    # -------------------------
    if not intents:
        intents.add("analysis")

    # =====================================================
    # PRIMARY selection
    # =====================================================
    PRIORITY = [
        "compare",
        "technical_intraday",
        "technical_indicator",
        "technical_risk",
        "technical_trend",
        "technical_full",
        "news",
        "sentiment",
        "price_now",
        "analysis"
    ]

    primary = next(i for i in PRIORITY if i in intents)
    secondary = list(intents - {primary})

    return {
        "primary": primary,
        "secondary": secondary,
        "flags": flags
    }

def detect_intent(query: str):
    # 1) rule-based first
    intent_pack = detect_multi_intent(query)
    return intent_pack["primary"]

def strip_other_tickers(text: str, allowed_ticker: str) -> str:
    """
    Remove any stock tickers other than `allowed_ticker` from the text.

    Rules:
    - Only remove ALL-CAPS ticker-like tokens (1â€“5 letters).
    - Preserve the allowed_ticker exactly.
    - Do NOT touch normal English words.
    """

    if not text:
        return text

    allowed = allowed_ticker.upper()


    common_words = {
        "A", "AN", "THE", "AND", "OR", "TO", "OF", "IN", "ON", "FOR", "WITH",
        "BY", "FROM", "AS", "AT", "IS", "ARE", "WAS", "WERE", "BE",
        "THIS", "THAT", "IT", "ITS", "US"
    }

    pattern = re.compile(r"\b[A-Z]{1,5}\b")

    def replacer(match):
        token = match.group(0)


        if token == allowed:
            return token


        if token in common_words:
            return token


        return ""

    cleaned = pattern.sub(replacer, text)


    cleaned = re.sub(r"\n\s*-\s*\n", "\n", cleaned)
    cleaned = re.sub(r"[ ]{2,}", " ", cleaned)
    cleaned = re.sub(r"\n{3,}", "\n\n", cleaned)

    return cleaned.strip()

"""# agent"""

import torch



def bull_agent_pm(ticker: str, tech: dict) -> str:
    return (
        f"{ticker} exhibits a constructive technical setup.\n\n"
        "Key points:\n"
        f"- Trend is {tech['trend'].lower()}, with price holding above key moving averages.\n"
        f"- Momentum remains supportive (RSI {tech['rsi14']:.0f}).\n"
        "- Pullbacks have been orderly, suggesting controlled demand."
    )

def bear_agent_pm(ticker: str, tech: dict) -> str:
    return (
        f"{ticker} faces near-term technical headwinds.\n\n"
        "Key risks:\n"
        f"- Trend classification is {tech['trend'].lower()}, limiting upside follow-through.\n"
        f"- Momentum signals are not decisive (RSI {tech['rsi14']:.0f}).\n"
        f"- Recent drawdowns (max {tech['max_drawdown']*100:.1f}%) highlight downside sensitivity."
    )

def risk_agent_pm(ticker: str, tech: dict) -> str:
    # risk label already computed in technical_summary_daily
    rating = tech["risk"]

    return (
        f"Key risks for {ticker}:\n"
        f"- Trend stability: {tech['trend']} structure may deteriorate if momentum weakens.\n"
        f"- Volatility exposure: recent drawdown reached {tech['max_drawdown']*100:.1f}%.\n"
        f"- Momentum risk: RSI at {tech['rsi14']:.0f} increases reversal sensitivity.\n\n"
        f"Risk rating = {rating}"
    )

def coordinator_agent(
    ticker: str,
    tech: dict,
    bull_text: str,
    bear_text: str,
    risk_text: str
) -> str:
    """
    PM-style final decision.
    Deterministic, explainable, no LLM.
    """

    # -------- scoring logic --------
    score = 0

    if tech["trend"] == "Bullish":
        score += 1
    elif tech["trend"] == "Weak":
        score -= 1

    if tech["rsi14"] >= 60:
        score += 0.5
    elif tech["rsi14"] <= 40:
        score -= 0.5

    if tech["risk"] == "Low":
        score += 0.5
    elif tech["risk"] == "High":
        score -= 0.5

    # -------- decision --------
    if score >= 1.5:
        rec = "Bullish"
        rationale = "Technical trend and momentum are aligned with contained risk."
    elif score <= -1.0:
        rec = "Bearish"
        rationale = "Weak trend and unfavorable momentum skew risks to the downside."
    else:
        rec = "Neutral"
        rationale = "Mixed technical signals limit conviction in either direction."

    return (
        f"Recommendation = {rec}\n"
        f"Rationale = {rationale}"
    )

"""# technical"""

"""# technical (FIXED & CLEAN VERSION)"""

import numpy as np
import pandas as pd
import yfinance as yf
from datetime import time

# =====================================================
# Global thresholds (CONSISTENT across system)
# =====================================================
RSI_POSITIVE = 55
RSI_OVERBOUGHT = 70

DD_LOW = -0.10
DD_MED = -0.25

MIN_DAILY_POINTS = 50
MIN_INTRADAY_POINTS = 30


# =====================================================
# Intraday data
# =====================================================
def get_intraday_1m(ticker: str):
    try:
        return yf.Ticker(ticker).history(period="1d", interval="1m")
    except Exception:
        return None


def intraday_technical_view(ticker: str):
    """
    Intraday (tactical) technical signals.
    NEVER overrides daily structure.
    """
    df = get_intraday_1m(ticker)

    if (
        df is None
        or df.empty
        or "Close" not in df
        or "Volume" not in df
        or len(df) < MIN_INTRADAY_POINTS
    ):
        return {
            "price_vs_vwap": None,
            "momentum": None,
            "bias": "Intraday data insufficient"
        }

    close = df["Close"]
    volume = df["Volume"]

    if volume.sum() == 0:
        return {
            "price_vs_vwap": None,
            "momentum": None,
            "bias": "Intraday data insufficient"
        }

    # VWAP
    vwap = (close * volume).cumsum() / volume.cumsum()
    last_price = close.iloc[-1].item()
    last_vwap = float(vwap.iloc[-1])

    price_vs_vwap = "Above" if last_price > last_vwap else "Below"

    # 30-min momentum
    ret_30m = float(close.iloc[-1] / close.iloc[-30] - 1)

    if ret_30m > 0.002:
        momentum = "Positive"
        bias = "Continuation"
    elif ret_30m < -0.002:
        momentum = "Negative"
        bias = "Pullback"
    else:
        momentum = "Neutral"
        bias = "Consolidation"

    return {
        "price_vs_vwap": price_vs_vwap,
        "momentum": momentum,
        "bias": bias
    }


# =====================================================
# Core technical computations
# =====================================================
def compute_rsi(close: pd.Series, period: int = 14) -> float:
    delta = close.diff()
    gain = delta.clip(lower=0)
    loss = -delta.clip(upper=0)

    avg_gain = gain.rolling(period).mean()
    avg_loss = loss.rolling(period).mean()

    rs = avg_gain / avg_loss.replace(0, np.nan)
    rsi = 100 - (100 / (1 + rs))

    return rsi.iloc[-1].item()


def compute_max_drawdown(close: pd.Series) -> float:
    peak = close.cummax()
    dd = close / peak - 1.0
    return dd.iloc[-1].item()


# =====================================================
# Daily technical summary (STRUCTURAL)
# =====================================================
def technical_summary_daily(ticker: str, period="6mo"):
    try:
        df = yf.download(
            ticker,
            period=period,
            progress=False,
            auto_adjust=False
        )
    except Exception:
        return None

    if df is None or df.empty or "Close" not in df:
        return None

    close = df["Close"].dropna()
    if len(close) < MIN_DAILY_POINTS:
        return {
            "status": "insufficient_data",
            "start": str(df.index.min().date()),
            "end": str(df.index.max().date())
        }

    ma20 = close.rolling(20).mean()
    ma50 = close.rolling(50).mean()

    last_close = close.iloc[-1].item()
    ma20_last  = ma20.iloc[-1].item()
    ma50_last  = ma50.iloc[-1].item()

    rsi14 = compute_rsi(close, 14) if len(close) >= 15 else None

    # limit drawdown window to recent history
    close_6m = close.iloc[-126:] if len(close) >= 126 else close
    mdd = compute_max_drawdown(close_6m)

    above_ma20 = last_close > ma20_last
    above_ma50 = last_close > ma50_last

    # ---------------- Trend ----------------
    if above_ma20 and above_ma50:
        trend = "Bullish"
    elif above_ma20:
        trend = "Improving"
    else:
        trend = "Weak"

    # ---------------- Momentum ----------------
    if rsi14 is None:
        momentum = "Unavailable"
    elif rsi14 >= RSI_OVERBOUGHT:
        momentum = "Overbought"
    elif rsi14 >= RSI_POSITIVE:
        momentum = "Positive"
    elif rsi14 >= 45:
        momentum = "Neutral"
    else:
        momentum = "Weak"

    # ---------------- Risk ----------------
    if rsi14 is not None and rsi14 >= 75:
        risk = "High"
    elif mdd <= DD_MED:
        risk = "High"
    elif mdd <= DD_LOW:
        risk = "Medium"
    else:
        risk = "Low"

    # ---------------- Daily Bias ----------------
    if trend == "Bullish" and momentum == "Positive" and risk != "High":
        daily_bias = "Constructive"
    elif momentum == "Overbought":
        daily_bias = "Extended"
    elif trend == "Weak":
        daily_bias = "Cautious"
    else:
        daily_bias = "Neutral"


    return {
        "status": "ok",
        "last_close": last_close,
        "ma20": ma20_last,
        "ma50": ma50_last,
        "rsi14": rsi14,
        "max_drawdown": mdd,
        "above_ma20": above_ma20,
        "above_ma50": above_ma50,
        "trend": trend,
        "momentum": momentum,
        "risk": risk,
        "daily_bias": daily_bias,
        "start": str(df.index.min().date()),
        "end": str(df.index.max().date())
    }


# =====================================================
# Formatter (RESEARCH STYLE)
# =====================================================
def format_technical_research_view(
    ticker: str,
    tech: dict,
    intra: dict | None = None
) -> str:

    if tech is None:
        return f"âš ï¸ Technical data unavailable for {ticker}."

    if tech.get("status") == "insufficient_data":
        return (
            f"ðŸ“ˆ Technical View for {ticker}\n\n"
            f"âš ï¸ Technical signals are limited due to short trading history "
            f"({tech.get('start')} â†’ {tech.get('end')})."
        )

    lines = []
    lines.append(f"ðŸ“ˆ Technical View for {ticker} ({tech['start']} â†’ {tech['end']})\n")

    # ---------------- Daily ----------------
    lines.append("[Daily â€“ Medium-term]")
    lines.append(
        f"â€¢ Trend: {tech['trend']} â€” "
        f"{'price above key moving averages' if tech['trend']!='Weak' else 'price below key moving averages'}."
    )

    if tech["rsi14"] is not None:
        lines.append(
            f"â€¢ Momentum: {tech['momentum']} â€” RSI at {tech['rsi14']:.1f}."
        )
    else:
        lines.append("â€¢ Momentum: Unavailable.")

    lines.append(
        f"â€¢ Risk: {tech['risk']} â€” max drawdown {tech['max_drawdown']*100:.1f}%."
    )

    lines.append(f"â€¢ Daily bias: {tech['daily_bias']}.")

    # ---------------- Intraday ----------------
    if intra and intra.get("bias") != "Intraday data insufficient":
        lines.append("\n[Intraday â€“ Short-term]")
        lines.append(f"â€¢ Price vs VWAP: {intra.get('price_vs_vwap')}")
        lines.append(f"â€¢ Momentum: {intra.get('momentum')}")
        lines.append(f"â€¢ Bias: {intra.get('bias')}")

    return "\n".join(lines)


# =====================================================
# Public entry
# =====================================================
def run_technical(ticker: str) -> str:
    tech = technical_summary_daily(ticker)
    intra = intraday_technical_view(ticker)

    return format_technical_research_view(
        ticker=ticker,
        tech=tech,
        intra=intra
    )

def detect_technical_subintent(query: str) -> str:
    q = query.lower()

    # -------------------------------------------------
    # 1) INTRADAY (highest priority)
    # -------------------------------------------------
    if any(k in q for k in [
        "intraday", "today", "short-term",
        "vwap", "opening", "session"
    ]):
        return "technical_intraday"

    # -------------------------------------------------
    # 2) EXPLAIN / DRIVERS (WHY)
    # -------------------------------------------------
    if any(k in q for k in [
        "why", "explain", "what is driving",
        "drivers", "behind", "factors"
    ]):
        return "technical_explain"

    # -------------------------------------------------
    # 3) MOVING AVERAGES
    # -------------------------------------------------
    if any(k in q for k in [
        "moving average", "moving averages",
        "above its key", "below its key",
        "ma20", "ma50", "20-day", "50-day"
    ]):
        return "technical_ma_check"

    # -------------------------------------------------
    # 4) INDICATORS
    # -------------------------------------------------
    if any(k in q for k in [
        "rsi", "macd", "momentum",
        "overbought", "oversold"
    ]):
        return "technical_indicator"

    # -------------------------------------------------
    # 5) RISK
    # -------------------------------------------------
    if any(k in q for k in [
        "risk", "drawdown", "volatility"
    ]):
        return "technical_risk"

    # -------------------------------------------------
    # 6) TREND / STRUCTURE
    # -------------------------------------------------
    if any(k in q for k in [
        "trend", "setup", "structure", "bias"
    ]):
        return "technical_trend"

    # -------------------------------------------------
    # 7) DEFAULT
    # -------------------------------------------------
    return "technical_full"

"""# news"""

#summary the paragraph for news
def summarize_news_paragraph(items: list, ticker: str, user_query: str, max_points: int = 4) -> str:
    """
    Turn Finnhub news items into one coherent paragraph tailored to the question.
    No LLM required: it extracts common themes + highlights likely price catalysts.
    """
    if not items:
        return f"I couldn't find any recent news items for {ticker} to summarize."

    # clean + keep top N (EARLY STOP for speed)
    clean = [it for it in items if isinstance(it, dict) and "error" not in it and it.get("datetime")]
    clean.sort(key=lambda it: it.get("datetime", 0), reverse=True)

    clean = clean[:max_points]

    if not clean:
      return f"I couldn't summarize because the news feed returned an error."

    sig = date_signature_sentence(clean, k=6)
    event_line = dominant_event_sentence(clean, ticker)

    # âœ… NEW: include one concrete detail from top item (makes days differ)
    concrete = extract_daily_event_hint(clean)

    # build bullet facts we can weave into a paragraph
    themes = set()

    # keywords used to detect price-relevant themes
    catalyst_keywords = {
        "legal/regulatory": ["court", "appeal", "ruling", "antitrust", "regulator", "doj", "ftc", "eu"],
        "analyst/valuation": ["rating", "upgrade", "downgrade", "price target", "valuation", "analyst"],
        "earnings/financials": ["earnings", "guidance", "revenue", "profit", "margin", "forecast", "eps"],
        "product/demand": ["iphone", "ipad", "mac", "demand", "sales", "shipments", "supply"],
        "macro/sector": ["ai", "rates", "dollar", "inflation", "tech sector", "etf"],
        "big holders/flow": ["buffett", "berkshire", "flows", "institutional"],
    }

    # detect themes from headlines + summaries
    for it in clean:
        text = f"{(it.get('headline') or '').lower()} {(it.get('summary') or '').lower()}"
        for theme, kws in catalyst_keywords.items():
            hits = sum(1 for k in kws if k in text)
            if hits >= 2:
                themes.add(theme)

    # ---- narrative summary (NO headline listing) ----
    theme_explanations = {
      "legal/regulatory": "legal and regulatory developments that could affect the companyâ€™s operations or key revenue streams",
      "analyst/valuation": "changes in analyst sentiment and valuation framing",
      "earnings/financials": "investor focus on earnings performance and forward guidance signals",
      "product/demand": "signals around demand for the companyâ€™s core products and end markets",
      "macro/sector": "broader technology-sector and macroeconomic trends influencing large-cap stocks",
      "big holders/flow": "positioning and sentiment among large institutional investors",
    }

    theme_sentences = [theme_explanations[t] for t in sorted(themes) if t in theme_explanations]
    theme_text = ""
    if theme_sentences:
        theme_text = "Secondary themes included: " + ", ".join(theme_sentences) + "."

    q = user_query.lower()
    if any(w in q for w in ["price", "stock", "moving", "why", "impact", "catalyst"]):
        conclusion = (
            f"Net: {ticker}â€™s move is likely tied to how traders re-priced these specific headlines today."
        )
    else:
        conclusion = "Overall, the news reflects how the company is being discussed today."

    paragraph = (
        f"Based on the most recent news for {ticker} on this date:\n\n"
        f"{sig}\n\n"
        f"{event_line}\n\n"
        f"Most concrete headline signal: {concrete}\n\n"
        f"{theme_text}\n\n"
        f"{conclusion}"
    )

    import textwrap
    return "\n\n".join(textwrap.fill(b, width=90) for b in paragraph.split("\n\n"))

#helper
def fix_mojibake(s: str) -> str:
    if not s:
        return s
    try:
        return s.encode("latin1").decode("utf-8")
    except Exception:
        return s

def extract_daily_event_hint(items):
    """
    Pulls a short event-specific hint from the most relevant item.
    """
    if not items:
        return ""

    it = items[0]  # most relevant item
    h = fix_mojibake((it.get("headline") or "").strip())
    s = fix_mojibake((it.get("summary") or "").strip())

    text = (h + ". " + s).strip()

    # Keep it short and neutral
    return text[:180].rstrip(".") + "."

#helper
import re
from collections import Counter

# Stronger stopwords + extra junk words that show up in finance headlines
_STOP = set("""
a an the and or but if to of in on for with by from is are was were be been being
this that these those it its as at into over under after before about
not no yes
i you we they he she them his her our your their
have has had having do does did doing
will would can could may might should
what why how when where which who
today yesterday tomorrow now just more most less very
""".split())

# Words that are common in market headlines but not informative as "topics"
_BLACKLIST = set("""
apple aapl stock stocks share shares price prices market markets investing investor investors
etf etfs fund funds qqq vgt
report reports says said update updates
buy point buy sell rating ratings target targets
year years month months week weeks day days
according amid expects expected said says report reports shares stock stocks
inc company companies group maker makers
eod income portfolio discover see another year added hold monthly stability quality
blessing bubble chasing riding well-positioned outperform going forward
""".split())

def date_signature_sentence(items, k=6):
    """
    Extract meaningful topic keywords from headlines+summaries.
    Filters stopwords + finance-generic words.
    """
    text = " ".join([
        fix_mojibake(it.get("headline","")) + " " + fix_mojibake(it.get("summary",""))
        for it in items
    ]).lower()

    # keep alphabetic words length>=3
    words = re.findall(r"[a-z]{3,}", text)

    cleaned = []
    for w in words:
        if w in _STOP:
            continue
        if w in _BLACKLIST:
            continue
        # remove very generic verbs that still sneak in
        if w in {"hit", "gets", "keep", "tumbled", "better", "going"}:
            continue
        cleaned.append(w)

    if not cleaned:
        return ""

    counts = Counter(cleaned)

    # Prefer words that appear at least twice if possible
    common = [w for w, c in counts.most_common() if c >= 2]
    top = common[:k] if common else [w for w, _ in counts.most_common(k)]

    return "Key topics mentioned include: " + ", ".join(top) + "."

#helper
def dominant_event_sentence(items, ticker: str) -> str:
    """
    Returns one short sentence describing the most prominent event/catalyst
    inferred from that day's headlines/summaries (no headline dumping).
    """
    text = " ".join([
        fix_mojibake(it.get("headline","")) + " " + fix_mojibake(it.get("summary",""))
        for it in items
    ]).lower()

    rules = [
        ("legal/regulatory", ["appeals court", "court", "ruling", "antitrust", "regulator", "doj", "ftc", "eu"],
         "attention centered on legal/regulatory developments"),
        ("analyst/valuation", ["upgrade", "downgrade", "price target", "rating", "analyst"],
         "coverage emphasized analyst views and valuation framing"),
        ("earnings/financials", ["earnings", "guidance", "revenue", "profit", "margin", "forecast", "outlook"],
         "investors focused on earnings and guidance signals"),
        ("product/demand", ["iphone", "ipad", "mac", "shipments", "sales", "demand", "supply"],
         "discussion highlighted product demand and supply signals"),
        ("macro/sector", ["rates", "inflation", "dollar", "recession", "sector", "etf", "nasdaq"],
         "the backdrop was broader macro and tech-sector sentiment"),
        ("big holders/flow", ["buffett", "berkshire", "stake", "flows", "institutional"],
         "stories referenced positioning by major holders and market flows"),
    ]

    hits = []
    for _, kws, sentence in rules:
        score = sum(text.count(k) for k in kws)
        hits.append((score, sentence))

    hits.sort(reverse=True, key=lambda x: x[0])
    if hits and hits[0][0] > 0:
        return f"On this date, {ticker} news was dominated by {hits[0][1]}."
    return f"On this date, coverage was mixed with no single dominant catalyst."

def wants_why_move(user_query: str) -> bool:
    q = (user_query or "").lower()
    return ("why" in q) and ("move" in q or "moving" in q) and ("stock" in q or "shares" in q)

#new

def wants_price_relevant_news(user_query: str) -> bool:
    q = user_query.lower()
    # if user mentions price movement / stock performance
    triggers = [
        "price", "stock", "shares", "move", "moving", "why", "down", "up",
        "drop", "rally", "selloff", "spike", "plunge", "reaction",
        "impact", "catalyst", "what happened"
    ]
    return any(t in q for t in triggers)

#new
import re
from datetime import date, datetime, timedelta

def extract_news_date(user_query: str):
    """
    Returns a date object if the query contains YYYY-MM-DD, else None.
    """
    m = re.search(r"\b(20\d{2})-(\d{2})-(\d{2})\b", user_query)
    if not m:
        return None
    y, mo, d = map(int, m.groups())
    try:
        return date(y, mo, d)
    except ValueError:
        return None

def _news_pipeline(items, ticker, user_query, asked_date=None):
    """
    Unified news filtering pipeline for both dated and non-dated queries.
    """

    # 0) basic guard
    items = [it for it in (items or []) if isinstance(it, dict) and "error" not in it]

    # 1) date filter (if applicable)
    if asked_date is not None:
        items = filter_items_to_date(items, asked_date)

    # 2) ticker relevance + company specificity
    items = [it for it in items if is_ticker_relevant(it, ticker)]
    items = [it for it in items if is_company_specific(it, ticker)]

    # 3) price-relevant filtering ONLY when user asks "why / move / impact"
    if wants_price_relevant_news(user_query) or wants_why_move(user_query):
        items = filter_price_relevant_news(
            items,
            ticker,
            max_items=12,
            min_score=1
        )
    else:
        items = items[:12]

    return items

"""# Sentiment"""

#new
from datetime import datetime, timezone
import numpy as np

SENTIMENT_MODE_LABEL = "label"
SENTIMENT_MODE_REASON = "reason"

class SentimentAnalyzer:
    """
    Dual-mode sentiment analyzer:
    - label: FinBERT aggregation (authoritative)
    - reason: fine-tuned LLM explanation (NO label override)
    """

    def __init__(self, finbert_pipeline, llm_model, tokenizer):
        self.finbert = finbert_pipeline
        self.llm = llm_model
        self.tokenizer = tokenizer

    # --------------------------------------------------
    # 1) Base sentiment via FinBERT (authoritative)
    # --------------------------------------------------
    def _analyze_label(self, news_items, ticker=None):
        if self.finbert is None:
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "dispersion": 0.0,
                "positive_count": 0,
                "negative_count": 0,
                "neutral_count": 0,
                "article_count": 0
            }

        if not news_items:
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "dispersion": 0.0,
                "positive_count": 0,
                "negative_count": 0,
                "neutral_count": 0,
                "article_count": 0
            }

        scores = []
        weights = []
        now_ts = datetime.now(timezone.utc).timestamp()

        for it in news_items:
            if not isinstance(it, dict) or "error" in it:
                continue

            text = ((it.get("headline","") or "") + ". " +
                    (it.get("summary","") or "")).strip()
            if not text:
                continue

            preds = self.finbert(text[:512])[0]
            best = max(preds, key=lambda d: d["score"])
            lab = best["label"].lower()
            conf = float(best["score"])

            sc = +conf if lab == "positive" else -conf if lab == "negative" else 0.0

            ts = it.get("datetime") or now_ts
            age_h = max(1.0, (now_ts - ts) / 3600.0)
            w = 1.0 / (age_h ** 0.5)

            scores.append(sc)
            weights.append(w)

        if not scores:
            return {
                "sentiment": "neutral",
                "score": 0.0,
                "dispersion": 0.0,
                "positive_count": 0,
                "negative_count": 0,
                "neutral_count": 0,
                "article_count": 0
            }

        scores = np.array(scores, dtype=float)
        weights = np.array(weights, dtype=float)

        avg = float(np.sum(scores * weights) / np.sum(weights))
        var = float(np.sum(weights * (scores - avg) ** 2) / np.sum(weights))
        disp = float(np.sqrt(var))

        pos = int(np.sum(scores > 0.1))
        neg = int(np.sum(scores < -0.1))
        neu = int(len(scores) - pos - neg)

        sentiment = "positive" if avg > 0.2 else "negative" if avg < -0.2 else "neutral"

        return {
            "sentiment": sentiment,   # âœ… matches format_sentiment_human()
            "score": avg,
            "dispersion": disp,
            "positive_count": pos,
            "negative_count": neg,
            "neutral_count": neu,
            "article_count": int(len(scores))
        }

    # --------------------------------------------------
    # 2) Reasoning via fine-tuned LLM (no label override)
    # --------------------------------------------------
    def _analyze_reason(self, news_items, ticker, base_sentiment):
        snippets = []
        for it in (news_items or [])[:6]:
            h = it.get("headline","") or ""
            s = it.get("summary","") or ""
            if h or s:
                snippets.append(f"- {h}. {s}".strip())

        if not snippets:
            return {
                "summary": "No sufficient recent news to explain sentiment.",
                "key_drivers": []
            }

        prompt = f"""
You are a financial analyst.

The overall sentiment for {ticker} is: {base_sentiment.upper()}.

Explain WHY this sentiment makes sense.

Rules:
- Do NOT change the sentiment label
- 1 short summary sentence
- 2â€“3 key drivers (bullets)

News:
{chr(10).join(snippets)}

Answer format:
Summary: <one sentence>
Drivers:
- <driver 1>
- <driver 2>
"""

        inputs = self.tokenizer(prompt, return_tensors="pt").to(self.llm.device)
        outputs = self.llm.generate(**inputs, max_new_tokens=120, do_sample=False)
        text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        summary = ""
        drivers = []
        for ln in text.splitlines():
            if ln.lower().startswith("summary:"):
                summary = ln.split(":", 1)[1].strip()
            if ln.strip().startswith("-"):
                drivers.append(ln.strip().lstrip("-").strip())

        return {
            "summary": summary or "Mixed signals across recent coverage.",
            "key_drivers": drivers[:3]
        }

    # --------------------------------------------------
    # 3) Public API
    # --------------------------------------------------
    def analyze(self, news_items, ticker=None, mode=SENTIMENT_MODE_LABEL):
        base = self._analyze_label(news_items, ticker)

        if mode == SENTIMENT_MODE_REASON:
            base["reasoning"] = self._analyze_reason(
                news_items,
                ticker or "",
                base["sentiment"]
            )

        return base

sentiment_analyzer = SentimentAnalyzer(finbert_pipeline=finbert, llm_model=model, tokenizer=tokenizer)

#new
def detect_sentiment_mode(user_query: str) -> str:
    q = user_query.lower()

    if any(k in q for k in [
        "why", "reason", "explain", "explanation",
        "because", "how", "detail", "explanation"
    ]):
        return SENTIMENT_MODE_REASON

    return SENTIMENT_MODE_LABEL

#new
def analyze_sentiment_dual(
    items,
    ticker,
    mode="label"
):
    base = sentiment_analyzer.analyze(items, ticker=ticker)

    if mode == "label":
        return base

    if mode == "reasoning":
        explanation = llm_sentiment_reasoning(
            items,
            ticker,
            model,
            tokenizer
        )
        return {
            "label": base["sentiment"],
            "score": base["score"],
            "reasoning": explanation
        }

    raise ValueError("mode must be 'label' or 'reasoning'")

#new
def run_sentiment(ticker: str) -> str:
    items = finnhub_company_news(
        ticker,
        days=7,
        max_items=20,
        strict=True
    )

    items = filter_price_relevant_news(
        items,
        ticker,
        max_items=10,
        min_score=3
    )

    sent = sentiment_analyzer.analyze(items, ticker=ticker)

    return format_sentiment_human(
        sent,
        ticker=ticker,
        when="recent"
    )

# ===============================
# Activate sentiment analyzer
# ===============================
sentiment_analyzer = SentimentAnalyzer(
    finbert_pipeline=finbert,
    llm_model=model,
    tokenizer=tokenizer
)

"""# financial_agent"""

# price change
def format_simple_table(rows):
    """
    rows: List[Dict[str, str]]
    """
    if not rows:
        return ""

    headers = list(rows[0].keys())
    col_widths = {
        h: max(len(h), *(len(str(r[h])) for r in rows))
        for h in headers
    }

    def _row_line(row):
        return " | ".join(
            str(row[h]).ljust(col_widths[h]) for h in headers
        )

    sep = "-+-".join("-" * col_widths[h] for h in headers)

    lines = []
    lines.append(_row_line(dict(zip(headers, headers))))
    lines.append(sep)

    for r in rows:
        lines.append(_row_line(r))

    return "\n".join(lines)

def finance_agent(user_query: str):
    q = (user_query or "").strip().lower()

    # =========================================================
    # ðŸš« 0) System / instruction guard
    # =========================================================
    if q.startswith(("you are", "rules:", "system:", "assistant:")):
        return "â„¹ï¸ System instruction detected. Please ask a stock-related question."

    # =========================================================
    # 1) Ticker extraction
    # =========================================================
    tickers = extract_tickers(user_query)
    ticker = pick_primary_ticker(tickers, query=user_query)

    if not tickers:
        return "âŒ Please include a valid stock ticker like AAPL, TSLA, NVDA."



    # =========================================================
    # 2) Intent routing (STRUCTURE > VIEW > LLM > RULE)
    # =========================================================

    intent_pack = detect_multi_intent(user_query)
    secondary    = intent_pack.get("secondary", [])
    flags        = intent_pack.get("flags", {})

    is_compare_query =(
        len(tickers) >= 2 and (
            "compare" in q or " vs " in q or "versus" in q
        ))
    # ---------- STRUCTURAL: compare ----------
    if is_compare_query:
        primary = "compare"

    # ---------- STRUCTURAL: dated / OHLCV (ðŸš¨ NEW, BEFORE LLM) ----------
    elif (
        extract_date(user_query) is not None
        or "yesterday" in q
    ) and any(k in q for k in ["open", "high", "low", "close", "price"]):
        primary = "ohlcv_on_date"

    elif "today" in q and any(k in q for k in ["open", "high", "low", "close", "price"]):
        primary = "ohlcv_today"

    # ---------- VIEWPOINT ----------
    elif any(k in q for k in ["bearish", "bear", "downside"]):
        primary = "bear"
    elif any(k in q for k in ["bullish", "bull", "upside"]):
        primary = "bull"
    elif "risk" in q:
        primary = "risk"

    # ---------- fallback to fine-tuned LLM ----------
    else:
        primary = predict_intent_with_epoch5(
            user_query,
            model=model,
            tokenizer=tokenizer
        )


    assert isinstance(primary, str)

    # ---------- technical sub-routing ----------
    if primary != "compare" and primary.startswith("technical"):
        primary = detect_technical_subintent(user_query)

    # ---------------------------------------------------------
    # Helper: format OHLCV fields
    # ---------------------------------------------------------
    def _format_field_values(field_values: dict, fields: list):
        parts = []
        for f in fields:
            v = field_values.get(f)
            if v is None:
                parts.append(f"{f}=N/A")
            elif f == "Volume":
                parts.append(f"{f}={int(v):,}")
            else:
                parts.append(f"{f}=${float(v):.2f}")
        return ", ".join(parts)

    # =========================================================
    # 3) Viewpoint-specific SINGLE agent
    # =========================================================
    if primary in {"bull", "bear", "risk"}:
        tech = technical_summary_daily(ticker)

        if not tech or tech.get("status") != "ok":
            return f"âš ï¸ Technical data unavailable for {ticker}."

        if primary == "bull":
            return bull_agent_pm(ticker, tech)
        if primary == "bear":
            return bear_agent_pm(ticker, tech)
        if primary == "risk":
            return risk_agent_pm(ticker, tech)


    # =========================================================
    # 4) Compare  (TABLE + PM VERDICT, FIXED)
    # =========================================================
    if primary == "compare":
        if len(tickers) < 2:
            return "âŒ Please provide two tickers to compare."

        tickers = tickers[:2]
        q = user_query.lower()
        # -------------------------------------------------
        # Compare type detection (NEW)
        # -------------------------------------------------
        compare_type = "technical"  # default

        if any(k in q for k in [
            "price", "performance", "return", "move", "moved", "change"
        ]):
            compare_type = "price"

        elif any(k in q for k in [
            "risk", "drawdown", "volatility"
        ]):
            compare_type = "risk"

        elif any(k in q for k in [
            "sentiment", "bullish", "bearish", "tone"
        ]):
            compare_type = "sentiment"
        # =================================================
        # PRICE-ONLY COMPARISON (NEW)
        # =================================================
        if compare_type == "price":
            rows = []

            for t in tickers:
                price_info = get_today_price_change(t)
                # price_info = {"price": float, "pct_change": float}

                rows.append({
                    "Ticker": t,
                    "Price": f"${price_info['price']:.2f}",
                    "Change": f"{price_info['pct_change']:+.2f}%"
                })

            out = []
            out.append("ðŸ“Š Price Performance Comparison (Today)\n")
            out.append(format_simple_table(rows))

            if len(rows) == 2:
                a, b = rows
                a_chg = float(a["Change"].replace("%", ""))
                b_chg = float(b["Change"].replace("%", ""))

                winner = a if a_chg > b_chg else b
                loser  = b if winner is a else a

                out.append("\nðŸ“Œ Relative Price Performance\n")
                out.append(f"Outperformed: {winner['Ticker']}")
                out.append(f"Underperformed: {loser['Ticker']}")

            return "\n".join(out)

        # -------------------------------------------------
        # What does user want to compare?
        # -------------------------------------------------
        wants_technical = any(
            i.startswith("technical") for i in secondary
        ) or any(k in q for k in [
            "technical", "setup", "trend", "structure",
            "rsi", "moving average", "ma", "risk", "drawdown"
        ])

        wants_sentiment = "sentiment" in secondary or any(
            k in q for k in ["sentiment", "bullish", "bearish", "tone"]
        )

        # -------------------------------------------------
        # Data containers
        # -------------------------------------------------
        rows = []
        tech_map = {}
        sent_map = {}

        # =================================================
        # Collect data ONLY (NO printing here)
        # =================================================
        for t in tickers:
            tech = technical_summary_daily(t) if wants_technical else None
            sent = None

            if wants_sentiment:
                items = finnhub_company_news(
                    t,
                    days=7,
                    max_items=20,
                    strict=True
                )
                items = filter_price_relevant_news(
                    items,
                    t,
                    max_items=10,
                    min_score=2
                )
                sent = sentiment_analyzer.analyze(items, ticker=t)

            if tech:
                tech_map[t] = tech
            if sent:
                sent_map[t] = sent

            score = score_technical_sentiment(tech or {}, sent or {})

            rows.append({
                "Ticker": t,
                "Trend": tech["trend"] if tech else "N/A",
                "RSI": f"{tech['rsi14']:.0f}" if tech and tech.get("rsi14") is not None else "N/A",
                "Max DD": f"{tech['max_drawdown']*100:.1f}%" if tech else "N/A",
                "Daily Bias": tech["daily_bias"] if tech else "N/A",
                "Sentiment": sent["sentiment"].capitalize() if sent else "N/A",
                "Score": score
            })

        # =================================================
        # Build output
        # =================================================
        out = []
        out.append("ðŸ“Š Technical & Sentiment Comparison\n")
        out.append(format_compare_table(rows))

        # =================================================
        # PM-style verdict
        # =================================================
        if len(rows) == 2:
            a, b = rows
            if a["Score"] != b["Score"]:
                winner = a if a["Score"] > b["Score"] else b
                loser  = b if winner is a else a

                out.append("\nðŸ“Œ PM-Style Relative Verdict\n")
                out.append(f"Preferred: {winner['Ticker']}")
                out.append(f"Relative Laggard: {loser['Ticker']}\n")

                out.append("Why:")

                # ---------- Technical explanation ----------
                if wants_technical:
                    out.append(
                        f"- {winner['Ticker']} has relatively stronger technical positioning "
                        f"(trend / drawdown / momentum composite)."
                    )

                # ---------- Sentiment explanation ----------
                if wants_sentiment:
                    w_sent = sent_map.get(winner["Ticker"], {})
                    l_sent = sent_map.get(loser["Ticker"], {})

                    if w_sent and l_sent:
                        out.append(
                            f"- {winner['Ticker']} sentiment is more favorable "
                            f"({w_sent.get('sentiment','N/A')} vs {l_sent.get('sentiment','N/A')})."
                        )

                        # optional: drivers if available
                        drivers = w_sent.get("drivers") or w_sent.get("key_topics")
                        if drivers:
                            out.append(
                                f"  Drivers: {', '.join(drivers[:3])}."
                            )


                out.append("\nWhat would change the view:")
                out.append(
                    f"- Momentum or trend deterioration in {winner['Ticker']}"
                )
                out.append(
                    f"- Confirmed technical improvement in {loser['Ticker']}"
                )
            else:
                out.append(
                    "\nðŸ“Œ PM Verdict: No clear relative preference at this time."
                )

        return "\n".join(out)

    # =========================================================
    # 5) News
    # =========================================================
    if primary == "news":
        asked_date = extract_news_date(user_query)
        ny = ZoneInfo("America/New_York")

        if asked_date is None and "today" in q:
            asked_date = datetime.now(ny).date()

        price_mode = wants_price_relevant_news(user_query)
        why_mode   = wants_why_move(user_query)

        if asked_date is not None:
            raw_items = finnhub_company_news_range(
                ticker, asked_date, asked_date + timedelta(days=1), max_items=50
            )
        else:
            raw_items = finnhub_company_news(
                ticker,
                days=7,
                max_items=50,
                strict=True
            )

        items = _news_pipeline(
            raw_items,
            ticker=ticker,
            user_query=user_query,
            asked_date=asked_date
        )

        if not items:
            if asked_date:
                return f"âš ï¸ No clearly {ticker}-specific news found on {asked_date}."
            return f"âš ï¸ No recent {ticker}-specific news."

        # -----------------------------------------
        # Summary paragraph
        # -----------------------------------------
        paragraph = summarize_news_paragraph(
            items[:8],
            ticker,
            user_query
        )

        # -----------------------------------------
        # WHY did the stock move?
        # -----------------------------------------
        if why_mode:
            mode = detect_sentiment_mode(user_query)

            sent = sentiment_analyzer.analyze(
                items[:12],
                ticker=ticker,
                mode=mode
            )

            sent_text = format_sentiment_human(
                sent,
                ticker=ticker,
                when=str(asked_date) if asked_date else "recent"
            )

            title = f"ðŸ§  Why {ticker} moved"
            if asked_date:
                title += f" on {asked_date}"

            return (
                f"{title}\n\n"
                f"{sent_text}\n\n"
                + format_news_items(items[:8])
                + "\n\nðŸ§  Summary:\n"
                + paragraph
            )

        # -----------------------------------------
        # Regular news response
        # -----------------------------------------
        return (
            format_news_items(items[:8])
            + "\n\nðŸ§  Summary:\n"
            + paragraph
        )

    # =========================================================
    # 6) Sentiment
    # =========================================================
    if primary == "sentiment":
        asked_date = extract_news_date(user_query)
        ny = ZoneInfo("America/New_York")

        # "today" support (optional)
        if asked_date is None and "today" in user_query.lower():
            asked_date = datetime.now(ny).date()

        # -------------------------
        # Fetch items (date-aware)
        # -------------------------
        if asked_date is not None:
            items = finnhub_company_news_range(
                ticker,
                asked_date,
                asked_date + timedelta(days=1),
                max_items=50
            )
            items = filter_items_to_date(items, asked_date)
        else:
            items = finnhub_company_news(ticker, days=7, max_items=50, strict=True)

        # keep only relevant/company-specific
        items = [it for it in (items or []) if isinstance(it, dict) and "error" not in it]
        items = [it for it in items if is_ticker_relevant(it, ticker)]
        items = [it for it in items if is_company_specific(it, ticker)]

        if not items:
            if asked_date:
                return f"âš ï¸ No clearly {ticker}-specific articles found on {asked_date}."
            return f"âš ï¸ No recent {ticker}-specific articles found."

        # optional: less strict so you don't filter everything out
        items = filter_price_relevant_news(items, ticker, max_items=12, min_score=1)

        mode = detect_sentiment_mode(user_query)  # label vs reason
        sent = sentiment_analyzer.analyze(items, ticker=ticker, mode=mode)

        when = str(asked_date) if asked_date else "recent"
        return format_sentiment_human(sent, ticker=ticker, when=when)

    # =========================================================
    # 7) Fundamentals
    # =========================================================
    if primary == "fundamentals":
        f = finnhub_fundamentals_basic(ticker)
        return format_fundamentals(f)

    # =========================================================
    # 8) OHLCV
    # =========================================================
    if primary == "ohlcv_on_date":
        date = extract_date(user_query)
        fields = extract_fields(user_query) or ["Close"]

        used_date, data = get_daily_ohlcv_fields_on_date(ticker, date, fields)
        return f"{ticker} on {used_date.date()}: " + _format_field_values(data, fields)

    if primary == "ohlcv_today":
        fields = extract_fields(user_query) or ["Price"]
        data = get_today_intraday_fields(ticker, fields)
        return f"{ticker} today: " + _format_field_values(data, fields)

    has_date = extract_date(user_query) is not None or "yesterday" in q

    if has_date and any(k in q for k in ["close", "closing", "price"]):
        primary = "ohlcv_on_date"

    # =========================================================
    # 9) Price
    # =========================================================


    if primary == "price_now":
        return price_now(ticker, mode="now")


    # =========================================================
    # 10) Technical (INTENT-AWARE)
    # =========================================================

    # -------- Intraday (HIGHEST priority) --------
    if primary == "technical_intraday":
        intra = intraday_technical_view(ticker)
        if not intra or intra.get("bias") == "Intraday data insufficient":
            return f"âš ï¸ Intraday technical data unavailable for {ticker}."

        return (
            f"â±ï¸ Intraday Technical Setup for {ticker}\n"
            f"â€¢ Price vs VWAP: {intra['price_vs_vwap']}\n"
            f"â€¢ Momentum: {intra['momentum']}\n"
            f"â€¢ Bias: {intra['bias']}"
        )


    # -------- Full technical research / WHY --------
    if primary in {"technical_full", "technical_explain"}:
        tech = technical_summary_daily(ticker)
        intra = intraday_technical_view(ticker)
        return format_technical_research_view(ticker, tech, intra)


    # -------- Moving average check --------
    if primary == "technical_ma_check":
        tech = technical_summary_daily(ticker)
        if not tech or tech.get("status") != "ok":
            return f"âš ï¸ Technical data unavailable for {ticker}."

        if tech["above_ma20"] and tech["above_ma50"]:
            return f"âœ… {ticker} is trading above its key moving averages."
        if tech["above_ma20"]:
            return f"âš ï¸ {ticker} is above its 20-day MA but below the 50-day MA."
        return f"âŒ {ticker} is trading below its key moving averages."


    # -------- Indicator-specific --------
    if primary == "technical_indicator":
        tech = technical_summary_daily(ticker)
        if not tech or tech.get("rsi14") is None:
            return f"âš ï¸ RSI data unavailable for {ticker}."

        rsi = tech["rsi14"]
        state = (
            "Overbought" if rsi >= 70 else
            "Positive" if rsi >= 55 else
            "Neutral" if rsi >= 45 else
            "Weak"
        )

        return f"ðŸ“ˆ RSI(14) for {ticker}: {rsi:.1f} â€” {state}."


    # -------- Trend / structure --------
    if primary == "technical_trend":
        tech = technical_summary_daily(ticker)
        if not tech:
            return f"âš ï¸ Technical data unavailable for {ticker}."

        return (
            f"ðŸ“Š Medium-term trend for {ticker}: {tech['trend']}.\n"
            f"Daily bias: {tech['daily_bias']}."
        )


    # -------- Risk profile --------
    if primary == "technical_risk":
        tech = technical_summary_daily(ticker)
        if not tech:
            return f"âš ï¸ Technical data unavailable for {ticker}."

        return (
            f"âš ï¸ Technical risk for {ticker}: {tech['risk']}.\n"
            f"Max drawdown (recent): {tech['max_drawdown']*100:.1f}%."
        )

    # =========================================================
    # 11) Fallback: Full multi-agent analysis
    # =========================================================
    tech = technical_summary_daily(ticker)
    bull = bull_agent_pm(ticker, tech)
    bear = bear_agent_pm(ticker, tech)
    risk = risk_agent_pm(ticker, tech)
    decision = coordinator_agent(ticker, tech, bull, bear, risk)

    return (
        f"ðŸ“Š Multi-Agent Analysis for {ticker}\n\n"
        f"ðŸŸ¢ Bull:\n{bull}\n\n"
        f"ðŸ”´ Bear:\n{bear}\n\n"
        f"âš ï¸ Risk:\n{risk}\n\n"
        f"ðŸ“Œ Final Recommendation:\n{decision}"
    )

if __name__ == "__main__":
    import sys

    query = " ".join(sys.argv[1:]).strip() or "AAPL price now"
    print(finance_agent(query))
